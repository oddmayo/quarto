# ðŸ¤– Web Scraping of Competitor Data with Generative AI

::: {align="center"}
![AI Powered](https://img.shields.io/badge/AI_Powered-Ollama-purple?style=for-the-badge&logo=ollama) ![Open Source](https://img.shields.io/badge/Open_Source-Community-blue?style=for-the-badge&logo=github) ![Databricks](https://img.shields.io/badge/Databricks-Enabled-orange?style=for-the-badge&logo=databricks)

**ðŸš€ *The Ultimate AI-Powered Competitive Intelligence Platform* ðŸš€**
:::

### **Performance Metrics** {#performance-metrics}

| ðŸŽ¯ **Metric** | ðŸ“ˆ **Value** | ðŸ’¡ **Impact** |
|-------------------------|-----------------------|-------------------------|
| **Automation Rate** | `95%` | Save 15+ hours/week |
| **Web sites** | `5 (11 planned, more on the way)` | Subaru and BMW for 3 countries |
| **Cost** | `0$` | No paid APIs, no proxies |
| **Compliance** | `100%` | Ethical & legal |

### **Problem Statement** {#problem-statement}

::: {.callout-note appearance="simple" icon=false}
## ðŸŒ What is Web Scraping?

**Web scraping** is the process of automatically extracting data from websites. Think of it as having a tireless assistant who visits web pages, reads the information, and organizes it into a structured format you can analyze.

:::

```{mermaid}
%%| fig-width: 7
flowchart LR
    subgraph WEB ["ðŸŒ The Web"]
        A[("ðŸ›’ Website A")]
        B[("ðŸ›’ Website B")]
        C[("ðŸ›’ Website C")]
    end
    
    subgraph SCRAPER ["ðŸ¤– Web Scraper"]
        D["Visits pages automatically"]
    end
    
    subgraph OUT ["ðŸ“Š Output"]
        E[("Organized Data")]
    end
    
    A --> D
    B --> D
    C --> D
    D --> E
    
    style WEB fill:#e3f2fd,stroke:#1976d2,color:#333
    style SCRAPER fill:#f3e5f5,stroke:#7b1fa2,color:#333
    style OUT fill:#e8f5e9,stroke:#388e3c,color:#333
    style A fill:#fff,stroke:#333,color:#333
    style B fill:#fff,stroke:#333,color:#333
    style C fill:#fff,stroke:#333,color:#333
    style D fill:#9b59b6,color:#fff,stroke:#7d3c98
    style E fill:#27ae60,color:#fff,stroke:#1e8449
```

::: {.panel-tabset}

### ðŸ‘¤ Manual Research

::: {layout="[30,70]"}
![](https://media.giphy.com/media/3o7TKRwpns23QMNNiE/giphy.gif){width="200" fig-align="center"}

| Aspect | Reality |
|--------|---------|
| **Speed** | Hours per website |
| **Scale** | Limited by human capacity |
| **Cost** | High (staff time) |
| **Accuracy** | Prone to errors |
| **Sustainability** | Not feasible long-term |
:::

### ðŸ”§ Traditional Scraping

::: {layout="[30,70]"}
![](https://media.giphy.com/media/ZVik7pBtu9dNS/giphy.gif){width="200" fig-align="center"}

| Aspect | Reality |
|--------|---------|
| **Speed** | Fast, but fragile |
| **Scale** | Breaks when sites change |
| **Cost** | Developer maintenance |
| **Accuracy** | Good until it breaks |
| **Sustainability** | Constant fixes needed |
:::

### ðŸ¤– AI-Powered (Our Solution)

::: {layout="[30,70]"}
![](https://media.giphy.com/media/RR32PdmXEwkuzZFKSa/giphy.gif){width="200" fig-align="center"}

| Aspect | Reality |
|--------|---------|
| **Speed** | âš¡ Fast & adaptive |
| **Scale** | âœ… Handles changes automatically |
| **Cost** | ðŸ’° Free (local LLMs) |
| **Accuracy** | ðŸŽ¯ Self-correcting |
| **Sustainability** | ðŸ”„ Minimal maintenance |
:::

:::

---

::: {.callout-important appearance="simple"}
## ðŸ’° Why Competitor Data Matters

In **parts pricing**, competitor data is important because it helps a business understand market trends, identify opportunities and threats, and make better strategic decisions to gain and sustain competitive advantage.
:::

::: {layout-ncol=3}
::: {.card}
### ðŸ—ï¸ Complex Structures
Websites are built differently â€” no two are alike
:::

::: {.card}
### ðŸ”„ Constant Changes
Layouts update frequently, breaking traditional scrapers
:::

::: {.card}
### ðŸ›¡ï¸ Anti-Bot Measures
Sites actively block automated data collection
:::
:::

Our team needed competitor prices **at scale**. Instead of expensive APIs or manual research, we built something **simpler, faster, and free** using modern AI tools.



# Cambios {#cambios}

-   No mencionar todas las pÃ¡ginas sino solo los mercados aprobados (Subaru y BMW de Ã©poca de Tiger): listo al inicio, falta al final en la evidencia.
-   ExplicaciÃ³n sencilla de web scraping: definiciÃ³n - (pasante esclavo sacando info, mentiras xd (manual) - librerÃ­as como requests o beautiful soup (insostenibilidad) - herramientas modernas LLMs):  LISTO
-   Enfatizar en las dificultades de scrapear - sostenibilidad y escalabilidad: LISTO
-   Mencionar proxy para reducir tiempos de ejecuciÃ³n - explicaciÃ³n sencilla como el ejemplo de los disfraces. AdemÃ¡s posible reducciÃ³n de costos al bajar la utilizaciÃ³n del cluster.
-   Simplificar parte de Solution Implementation y todo en general
-   Performance metrics al inicio con mÃ¡s mÃ©tricas.
-   Agregar mÃ¡s grafiquitos.

------------------------------------------------------------------------

## Table of Contents {#table-of-contents}

-   [ðŸ¤– Web Scraping of Competitor Data with Generative AI](#-web-scraping-of-competitor-data-with-generative-ai)
    -   [**Problem Statement**](#problem-statement)
    -   [**Performance Metrics**](#performance-metrics)
-   [Cambios](#cambios)
    -   [Table of Contents](#table-of-contents)
    -   [1. The Tools](#1-the-tools)
    -   [2. Features](#2-features)
    -   [3. The Plan](#3-the-plan)
    -   [4. Solution Implementation](#4-solution-implementation)
        -   [4.1 Setup](#41-setup)
        -   [4.2 Databricks Workspace](#42-databricks-workspace)
            -   [4.2.1 *oem_codes*](#421-oem_codes)
            -   [4.2.2 *scraper_calls*](#422-scraper_calls)
            -   [4.2.3 *markets*](#423-markets)
            -   [4.2.4 *mystery shopper and third party scrapper*](#424-mystery-shopper-and-third-party-scrapper)
        -   [4.3 AI scrapper logic](#43-ai-scrapper-logic)
        -   [4.4 AI scrapper class](#44-ai-scrapper-class)
    -   [5. Final table](#5-final-table)
    -   [6. Databricks Job](#6-databricks-job)
    -   [7. Our own scraping AI Assistant](#7-our-own-scraping-ai-assistant)
    -   [8. Next Steps](#8-next-steps)

------------------------------------------------------------------------

## 1. The Tools

::: {.callout-tip appearance="simple" icon=false}
## ðŸŽ¯ One person can now manage dozens of websites with minimal maintenance
Previously, entire teams were dedicated to competitor data collection. Our solution combines **two powerful open-source tools**:
:::

::: {layout-ncol=2}

::: {.card}
### ![Crawl4AI](https://img.shields.io/badge/Crawl4AI-3776AB?style=for-the-badge&logo=python&logoColor=white)

**AI-ready web scraping library**

- Extracts data using traditional methods + LLMs
- Handles complex website structures
- [GitHub Repository](https://github.com/unclecode/crawl4ai)
:::

::: {.card}
### ![Ollama](https://img.shields.io/badge/Ollama-412991?style=for-the-badge&logo=ollama&logoColor=white)

**Run LLMs locally â€” no API costs**

- Free access to powerful AI models
- Runs on our Databricks cluster
- [Browse Models](https://ollama.com/search)
:::

:::

::: {.callout-note appearance="minimal"}
Both tools are **open-source**, supported by large communities, and safe for enterprise use.
:::


## 2. Features

Here's what our AI-powered scraping solution can do:

| Feature | Description | Status |
|----------------------|------------------------------|--------------------|
| ðŸ•·ï¸ **Smart scraping** | Transform manual research into automated tasks | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ðŸ”§ **Resilient** | Robust to website changes | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| âš¡ **Performance** | Handle multiple competitors simultaneously | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ðŸ›¡ï¸ **Compliance** | Built-in ethical guidelines and rate limiting | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ðŸ“Š **Pricing Simulator** | Feed competitor data into our pricing optimization tool | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ðŸ’¡ **Coding assistant** | Local AI assistant for fast development iteration | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ðŸ—“ï¸ **Monthly monitoring** | Build historical database of competitor prices | ![In Progress](https://img.shields.io/badge/In_Progress-yellow?style=flat-square) |
| ðŸš€ **Scalability** | Solution applicable to any team in Inchcape | ![Planned](https://img.shields.io/badge/Planned-blue?style=flat-square) |
| ðŸ¤– **AI analysis** | Track competitor pricing behaviors with AI | ![Planned](https://img.shields.io/badge/Planned-blue?style=flat-square) |

------------------------------------------------------------------------

## 3. The Plan

First, we study suitable websites to ensure that they are relevant (i.e. not accessories or vehicles) and allow data extraction. We then follow this structure to extract competitors' data on parts pricing.

```{mermaid}
%%| fig-width: 8
flowchart TB
    subgraph INPUT ["ðŸ“¥ Data Sources"]
        K[(Azure Delta Lake)]
        J[âš™ï¸ Databricks Job]
        A[ðŸŒ Competitor Websites]
    end
    
    subgraph PROCESS ["ðŸ”„ Processing"]
        direction TB
        L[ERP Classification]
        I[OEM Part Numbers]
        
        subgraph FILTER ["Website Filtering"]
            B[âŒ Non-suitable]
            C[âœ… Suitable]
        end
        
        subgraph EXTRACT ["Extraction Methods"]
            F[ðŸ”§ CSS Extraction]
            G[ðŸ¤– AI Extraction]
        end
        
        M[ðŸ§  Intelligent Scraper Class]
    end
    
    subgraph OUTPUT ["ðŸ“¤ Output"]
        H[(Competitors Data Table)]
        N[ðŸ“Š Pricing Simulator Tool]
    end
    
    J --> A
    K --> L
    L --> I
    A --> B
    A --> C
    C --> F
    C --> G
    F --> M
    G --> M
    I --> M
    M --> H
    H --> N
    
    style INPUT fill:#e8f4fd,stroke:#1a73e8
    style PROCESS fill:#fef7e0,stroke:#f9ab00
    style OUTPUT fill:#e6f4ea,stroke:#34a853
    style FILTER fill:#fff3e0,stroke:#ff9800,color:#333
    style EXTRACT fill:#e3f2fd,stroke:#2196f3,color:#333
    style M fill:#9b59b6,color:#fff,stroke:#7d3c98
    style H fill:#27ae60,color:#fff,stroke:#1e8449
    style J fill:#fff,stroke:#333,color:#333
    style K fill:#fff,stroke:#333,color:#333
    style A fill:#fff,stroke:#333,color:#333
    style L fill:#fff,stroke:#333,color:#333
    style I fill:#fff,stroke:#333,color:#333
    style B fill:#ffebee,stroke:#c62828,color:#333
    style C fill:#e8f5e9,stroke:#2e7d32,color:#333
    style F fill:#fff,stroke:#333,color:#333
    style G fill:#fff,stroke:#333,color:#333
    style N fill:#fff,stroke:#333,color:#333
```

## 4. Solution Implementation

::: {.callout-tip appearance="simple" icon=false}
## ðŸ’¡ The Key Insight
Traditional scraping requires manually finding **44+ HTML elements** across our websites. Our AI reads the page and extracts what we need â€” automatically.
:::

### How It Works

::: {layout-ncol=2}

::: {.card}
#### âŒ Traditional Way
1. Inspect website HTML manually
2. Find the exact tag for each element
3. Write custom code for each site
4. **Repeat when site changes** ðŸ˜©
:::

::: {.card}
#### âœ… Our AI Way  
1. Tell the AI what data you need
2. AI reads the page and finds it
3. Works across different sites
4. **Adapts to changes automatically** ðŸŽ‰
:::

:::

### The Hybrid Approach

We combine **both methods** for the best results:

```{mermaid}
%%| fig-width: 6
flowchart LR
    A[ðŸŒ Website] --> B{Complex?}
    B -->|Yes| C[ðŸ¤– AI extracts data directly]
    B -->|No| D[ðŸ¤– AI finds HTML tags]
    D --> E[âš¡ Fast CSS extraction]
    C --> F[ðŸ“Š Structured Data]
    E --> F
    
    style A fill:#fff,stroke:#333,color:#333
    style B fill:#fff3e0,stroke:#ff9800,color:#333
    style C fill:#e8f5e9,stroke:#388e3c,color:#333
    style D fill:#e3f2fd,stroke:#1976d2,color:#333
    style E fill:#e3f2fd,stroke:#1976d2,color:#333
    style F fill:#27ae60,color:#fff,stroke:#1e8449
```

This gives us **robustness** (AI handles changes) + **speed** (CSS extraction is fast).

::: {.callout-note collapse="true"}
## ðŸ”§ Technical Details (click to expand)

**Setup commands:**
```bash
curl -fsSL https://ollama.com/install.sh | sh    # Install Ollama
pip install -U crawl4ai                           # Install Crawl4AI
ollama pull qwen2.5:3b                            # Download AI model
```

**What we extract from each product:**
- Part name
- List price (MSRP)
- Sale price

**Databricks workspace structure:**

<img src="img/folders.png" width="200"/>

| Folder | Purpose |
|--------|---------|
| `oem_codes` | Consolidates part numbers from different ERPs |
| `scraper_calls` | Core scraping automation logic |
| `markets` | Website-specific notebooks |

:::

## 5. Final table

This unique table contains information on every brand and country. You can view it here:

```         
development.parts_pricing_feature_store_americas_dev.competitors_ppo_global_v2
```

| **MANU_MATNR** | **oem_name** | **product_hierarchy** | **list_price** | **sale_price** | **url** | **scraped_date** | **competitor_name** | **currency** | **country** | **brand** | **source** |
|------|------|------|------|------|------|------|------|------|------|------|------|
| 806750050 | Automatic Transmission Oil Pump Seal | 100005692 | 9.87 | 7.81 | https://www.subaruparts.com/oem-parts/subaru-automatic-transmission-oil-pump-seal-806750050?c=bD0xJm49U2VhcmNoIFJlc3VsdHM%3D | 2025-11-01 | subaruparts.com | USD | CHILE | SUBARU | scraper |

## 6. Databricks Job

This scraping process runs automatically once a month via a scheduled job. The **11 current webpages** are as follows:

<img src="img/job_webpages.png" width="500"/>

\
The latest run took 106 hours â€” just over **four days**. Not bad, considering we made requests at prudent intervals and did not use a proxy for almost 100,000 parts in total. We've been making steady improvements each week.

\
<img src="img/job_time.png" width="500"/>

We have a **scraping priority**: we use the parts segmentation table to first scrape the parts with AA priority, then AB, then BA, and so on.

If an error occurs during execution, the scrapper is provided with a **checkpoint capability**. In that case, all progress is saved, and the ability to resume makes it easy to continue from the last OEM part scraped.

## 7. Our own scraping AI Assistant

The complexity of the code for this scraper increases with the number of web pages included and with each small feature. Even if the code offers an elegant solution, it will be overwhelming for a new data scientist.

For this reason, **we have created our own assistant** using the free tier of **Google Gemini**. This assistant considers all the code, not only from Databricks, but also from the official GitHub repository of the Crawl4AI library. With this personalised tool, we can iterate quickly and make modifications with minimal hallucinations. Both ChatGPT and the Databricks assistant struggle with this code.

The prompt used is as follows:

``` python
from llama_index.core.prompts import PromptTemplate
qa_prompt_tmpl_str = (
    "Context information is below.\n"
    "---------------------\n"
    "{context_str}\n"
    "---------------------\n"
    "Given the context above, answer the query in a step-by-step manner. "
    "Include code snippets when relevant. If you don't know the answer, say 'I don't know!'.\n"
    "Query: {query_str}\n"
    "Answer: "
)
qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)
```

This is what the assistant can do:

``` python
response = query_engine.query('Provide a working example of multiple url crawling with magic mode')
display(Markdown(str(response)))
```

```         
2025-10-20 20:33:11.323 | INFO     | py4j.clientserver:wait_for_commands:600 | Received command c on object id p0
To crawl multiple URLs with Magic Mode, you can iterate through a list of URLs and call the arun method for each one within the AsyncWebCrawler context.

Here's a step-by-step example:

1. Import necessary libraries: You'll need AsyncWebCrawler from crawl4ai and asyncio to run the asynchronous code.

import asyncio
from crawl4ai import AsyncWebCrawler

2. Define a list of URLs: Create a list containing the URLs you want to crawl.

urls_to_crawl = [
    "https://www.reuters.com/markets/us/global-markets-view-usa-pix-2024-08-29/",
    "https://crawl4ai.com",
    "https://janineintheworld.com/places-to-visit-in-central-mexico"
]
3. Create an asynchronous function: This function will contain the crawling logic.
...
```

Now, with **GitHub Copilot** integrated into our local environments, we can provide it with the same context and receive an even better assistant that can modify code on the fly!

## 8. Next Steps

-   Use the AI models to study the patterns and behaviours of multiple competitors.
-   Use this tool for other scraping needs across Inchcape.

\
\
\*This document was created with the help of GitHub Copilot.