---
title: "ğŸ¤– Web Scraping of Competitor Data with Generative AI"
subtitle: | 
    **ğŸš€ *The Ultimate AI-Powered Competitive Intelligence Platform* ğŸš€** 

    ![AI Powered](https://img.shields.io/badge/AI_Powered-Ollama-purple?style=for-the-badge&logo=ollama) ![Open Source](https://img.shields.io/badge/Open_Source-Community-blue?style=for-the-badge&logo=github) ![Databricks](https://img.shields.io/badge/Databricks-Enabled-orange?style=for-the-badge&logo=databricks)


abstract-title: Important
abstract: This is an interactive document! Don't forget to explore the expandable sections and embedded content.
author: Camilo MayorquÃ­n (Data Science Team)
date: November 28 2025


title-block-banner: true
format:
  html:
    theme: cosmo
    toc: true
    toc-location: left
    toc-depth: 2
    toc-title: "Contents"
    smooth-scroll: true
---

### **Performance Metrics** {#performance-metrics}

| ğŸ¯ **Metric** | ğŸ“ˆ **Value** | ğŸ’¡ **Impact** |
|-------------------------|-----------------------|-------------------------|
| **Automation Rate** | `95%` | Save 15+ hours/week |
| **Web sites** | `5 (11 planned, more on the way)` | Subaru and BMW for 3 countries |
| **Cost** | `0$` | No paid APIs, no proxies |
| **Compliance** | `100%` | Ethical & legal |

### **Problem Statement** {#problem-statement}

::: {.callout-note appearance="simple" icon=false}
## ğŸŒ What is Web Scraping?

**Web scraping** is the process of automatically extracting data from websites. Think of it as having a tireless assistant who visits web pages, reads the information, and organizes it into a structured format you can analyze.

:::

```{mermaid}
%%| fig-width: 7
flowchart LR
    subgraph WEB ["ğŸŒ The Web"]
        A[("ğŸ›’ Website A")]
        B[("ğŸ›’ Website B")]
        C[("ğŸ›’ Website C")]
    end
    
    subgraph SCRAPER ["ğŸ¤– Web Scraper"]
        D["Visits pages automatically"]
    end
    
    subgraph OUT ["ğŸ“Š Output"]
        E[("Organized Data")]
    end
    
    A --> D
    B --> D
    C --> D
    D --> E
    
    style WEB fill:#e3f2fd,stroke:#1976d2,color:#333
    style SCRAPER fill:#f3e5f5,stroke:#7b1fa2,color:#333
    style OUT fill:#e8f5e9,stroke:#388e3c,color:#333
    style A fill:#fff,stroke:#333,color:#333
    style B fill:#fff,stroke:#333,color:#333
    style C fill:#fff,stroke:#333,color:#333
    style D fill:#9b59b6,color:#fff,stroke:#7d3c98
    style E fill:#27ae60,color:#fff,stroke:#1e8449
```

::: {.panel-tabset}

### ğŸ‘¤ Manual Research

::: {layout="[30,70]"}
![](https://media.giphy.com/media/3o7TKRwpns23QMNNiE/giphy.gif){width="200" fig-align="center"}

| Aspect | Reality |
|--------|---------|
| **Speed** | Hours per website |
| **Scale** | Limited by human capacity |
| **Cost** | High (staff time) |
| **Accuracy** | Prone to errors |
| **Sustainability** | Not feasible long-term |
:::

### ğŸ”§ Traditional Scraping

::: {layout="[30,70]"}
![](https://media.giphy.com/media/ZVik7pBtu9dNS/giphy.gif){width="200" fig-align="center"}

| Aspect | Reality |
|--------|---------|
| **Speed** | Fast, but fragile |
| **Scale** | Breaks when sites change |
| **Cost** | Developer maintenance |
| **Accuracy** | Good until it breaks |
| **Sustainability** | Constant fixes needed |
:::

### ğŸ¤– AI-Powered (Our Solution)

::: {layout="[30,70]"}
![](https://media.giphy.com/media/RR32PdmXEwkuzZFKSa/giphy.gif){width="200" fig-align="center"}

| Aspect | Reality |
|--------|---------|
| **Speed** | âš¡ Fast & adaptive |
| **Scale** | âœ… Handles changes automatically |
| **Cost** | ğŸ’° Free (local LLMs) |
| **Accuracy** | ğŸ¯ Self-correcting |
| **Sustainability** | ğŸ”„ Minimal maintenance |
:::

:::

---

::: {.callout-important appearance="simple"}
## ğŸ’° Why Competitor Data Matters

In **parts pricing**, competitor data is important because it helps a business understand market trends, identify opportunities and threats, and make better strategic decisions to gain and sustain competitive advantage.
:::

::: {.callout-warning appearance="simple" icon=false}
## âš ï¸ But scraping is hard...

::: {layout-ncol=3}
::: {.card}
### ğŸ—ï¸ Complex Structures
Websites are built differently â€” no two are alike
:::

::: {.card}
### ğŸ”„ Constant Changes
Layouts update frequently, breaking traditional scrapers
:::

::: {.card}
### ğŸ›¡ï¸ Anti-Bot Measures
Sites actively block automated data collection
:::
:::
:::

Our team needed competitor prices **at scale**. Instead of expensive APIs or manual research, we built something **simpler, faster, and free** using modern AI tools.


------------------------------------------------------------------------

## 1. The Tools

::: {.callout-tip appearance="simple" icon=false}
## ğŸ¯ One person can now manage dozens of websites with minimal maintenance
Previously, entire teams were dedicated to competitor data collection. Our solution combines **two powerful open-source tools**:
:::

::: {layout-ncol=2}

::: {.card}
### ![Crawl4AI](https://img.shields.io/badge/Crawl4AI-3776AB?style=for-the-badge&logo=python&logoColor=white)

**AI-ready web scraping library**

- Extracts data using traditional methods + LLMs
- Handles complex website structures
- [GitHub Repository](https://github.com/unclecode/crawl4ai)
:::

::: {.card}
### ![Ollama](https://img.shields.io/badge/Ollama-412991?style=for-the-badge&logo=ollama&logoColor=white)

**Run LLMs locally â€” no API costs**

- Free access to powerful AI models
- Runs on our Databricks cluster
- [Browse Models](https://ollama.com/search)
:::

:::

::: {.callout-note appearance="minimal"}
Both tools are **open-source**, supported by large communities, and safe for enterprise use.
:::


## 2. Features

Here's what our AI-powered scraping solution can do:

| Feature | Description | Status |
|----------------------|------------------------------|--------------------|
| ğŸ•·ï¸ **Smart scraping** | Transform manual research into automated tasks | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ğŸ”§ **Resilient** | Robust to website changes | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| âš¡ **Performance** | Handle multiple competitors simultaneously | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ğŸ›¡ï¸ **Compliance** | Built-in ethical guidelines and rate limiting | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ğŸ“Š **Pricing Simulator** | Feed competitor data into our pricing optimization tool | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ğŸ’¡ **Coding assistant** | Local AI assistant for fast development iteration | ![Ready](https://img.shields.io/badge/Ready-success?style=flat-square) |
| ğŸ—“ï¸ **Monthly monitoring** | Build historical database of competitor prices | ![In Progress](https://img.shields.io/badge/In_Progress-yellow?style=flat-square) |
| ğŸš€ **Scalability** | Solution applicable to any team in Inchcape | ![Planned](https://img.shields.io/badge/Planned-blue?style=flat-square) |
| ğŸ¤– **AI analysis** | Track competitor pricing behaviors with AI | ![Planned](https://img.shields.io/badge/Planned-blue?style=flat-square) |

------------------------------------------------------------------------

## 3. The Plan

First, we study suitable websites to ensure that they are relevant (i.e. not accessories or vehicles) and allow data extraction. We then follow this structure to extract competitors' data on parts pricing.

```{mermaid}
%%| fig-width: 8
flowchart TB
    subgraph INPUT ["ğŸ“¥ Data Sources"]
        K[(Azure Delta Lake)]
        J[âš™ï¸ Databricks Job]
        A[ğŸŒ Competitor Websites]
    end
    
    subgraph PROCESS ["ğŸ”„ Processing"]
        direction TB
        L[ERP Classification]
        I[OEM Part Numbers]
        
        subgraph FILTER ["Website Filtering"]
            B[âŒ Non-suitable]
            C[âœ… Suitable]
        end
        
        subgraph EXTRACT ["Extraction Methods"]
            F[ğŸ”§ CSS Extraction]
            G[ğŸ¤– AI Extraction]
        end
        
        M[ğŸ§  Intelligent Scraper Class]
    end
    
    subgraph OUTPUT ["ğŸ“¤ Output"]
        H[(Competitors Data Table)]
        N[ğŸ“Š Pricing Simulator Tool]
    end
    
    J --> A
    K --> L
    L --> I
    A --> B
    A --> C
    C --> F
    C --> G
    F --> M
    G --> M
    I --> M
    M --> H
    H --> N
    
    style INPUT fill:#e8f4fd,stroke:#1a73e8
    style PROCESS fill:#fef7e0,stroke:#f9ab00
    style OUTPUT fill:#e6f4ea,stroke:#34a853
    style FILTER fill:#fff3e0,stroke:#ff9800,color:#333
    style EXTRACT fill:#e3f2fd,stroke:#2196f3,color:#333
    style M fill:#9b59b6,color:#fff,stroke:#7d3c98
    style H fill:#27ae60,color:#fff,stroke:#1e8449
    style J fill:#fff,stroke:#333,color:#333
    style K fill:#fff,stroke:#333,color:#333
    style A fill:#fff,stroke:#333,color:#333
    style L fill:#fff,stroke:#333,color:#333
    style I fill:#fff,stroke:#333,color:#333
    style B fill:#ffebee,stroke:#c62828,color:#333
    style C fill:#e8f5e9,stroke:#2e7d32,color:#333
    style F fill:#fff,stroke:#333,color:#333
    style G fill:#fff,stroke:#333,color:#333
    style N fill:#fff,stroke:#333,color:#333
```

## 4. Solution Implementation

::: {.callout-tip appearance="simple" icon=false}
## ğŸ’¡ The Key Insight
Traditional scraping requires manually finding **44+ HTML elements** across our websites. Our AI reads the page and extracts what we need â€” automatically.
:::

### How It Works

::: {layout-ncol=2}

::: {.card}
#### âŒ Traditional Way
1. Inspect website HTML manually
2. Find the exact tag for each element
3. Write custom code for each site
4. **Repeat when site changes** ğŸ˜©
:::

::: {.card}
#### âœ… Our AI Way  
1. Tell the AI what data you need
2. AI reads the page and finds it
3. Works across different sites
4. **Adapts to changes automatically** ğŸ‰
:::

:::

### The Hybrid Approach

We combine **both methods** for the best results:

```{mermaid}
%%| fig-width: 6
flowchart LR
    A[ğŸŒ Website] --> B{Complex?}
    B -->|Yes| C[ğŸ¤– AI extracts data directly]
    B -->|No| D[ğŸ¤– AI finds HTML tags]
    D --> E[âš¡ Fast CSS extraction]
    C --> F[ğŸ“Š Structured Data]
    E --> F
    
    style A fill:#fff,stroke:#333,color:#333
    style B fill:#fff3e0,stroke:#ff9800,color:#333
    style C fill:#e8f5e9,stroke:#388e3c,color:#333
    style D fill:#e3f2fd,stroke:#1976d2,color:#333
    style E fill:#e3f2fd,stroke:#1976d2,color:#333
    style F fill:#27ae60,color:#fff,stroke:#1e8449
```

This gives us **robustness** (AI handles changes) + **speed** (CSS extraction is fast).

Data from **mystery shoppers** and **third-party scrapers** is also included.

::: {.callout-note collapse="true"}
## ğŸ”§ Technical Details (click to expand)

**Setup commands:**
```bash
curl -fsSL https://ollama.com/install.sh | sh    # Install Ollama
pip install -U crawl4ai                           # Install Crawl4AI
ollama pull qwen2.5:3b                            # Download AI model
```

**What we extract from each product:**
- Part name
- List price (MSRP)
- Sale price

**Databricks workspace structure:**

<img src="img/folders.png" width="200"/>

| Folder | Purpose |
|--------|---------|
| `oem_codes` | Consolidates part numbers from different ERPs |
| `scraper_calls` | Core scraping automation logic |
| `markets` | Website-specific notebooks |
| `mystery_shopper` | Data obatined from research |
| `third_party` | Data obtained from third party scrapers |

:::



## 5. Final Result

::: {.callout-tip appearance="simple" icon=false}
## ğŸ“Š One Table to Rule Them All
All competitor data flows into a **single, unified table** â€” regardless of brand, country, or source.
:::

::: {layout-ncol=4}
::: {.card style="text-align: center;"}
### ğŸ·ï¸
**Part Info**
Name & codes
:::
::: {.card style="text-align: center;"}
### ğŸ’°
**Pricing**
List & sale price
:::
::: {.card style="text-align: center;"}
### ğŸŒ
**Location**
Country & currency
:::
::: {.card style="text-align: center;"}
### ğŸ“…
**Tracking**
Date & source
:::
:::

**Sample row from the table:**

| Part Code | Part Name | List Price | Sale Price | Competitor | Country |
|-----------|-----------|------------|------------|------------|---------|
| 806750050 | Transmission Oil Pump Seal | $9.87 | $7.81 | subaruparts.com | Chile ğŸ‡¨ğŸ‡± |

::: {.callout-note collapse="true"}
## ğŸ“ Table Location (click to expand)
```
Name: competitors_ppo_global_v2
Location: Azure Delta Lake
```
Full columns: `MANU_MATNR`, `oem_name`, `product_hierarchy`, `list_price`, `sale_price`, `url`, `scraped_date`, `competitor_name`, `currency`, `country`, `brand`, `source`
:::

### ğŸ—ºï¸ Geographic Coverage

```{=html}
<div id="map" style="height: 400px; width: 100%; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);"></div>
<link rel="stylesheet" href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css" />
<script src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function() {
    var map = L.map('map').setView([10, -20], 2);
    
    L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {
        attribution: 'Â© OpenStreetMap'
    }).addTo(map);
    
    var bmwIcon = L.divIcon({
        className: 'custom-div-icon',
        html: "<div style='background-color:#0066b1;color:white;padding:5px 10px;border-radius:5px;font-weight:bold;font-size:12px;'>BMW</div>",
        iconSize: [50, 20],
        iconAnchor: [25, 10]
    });
    
    var subaruIcon = L.divIcon({
        className: 'custom-div-icon',
        html: "<div style='background-color:#013C74;color:white;padding:5px 10px;border-radius:5px;font-weight:bold;font-size:12px;'>Subaru</div>",
        iconSize: [60, 20],
        iconAnchor: [30, 10]
    });
    
    var dercoIcon = L.divIcon({
        className: 'custom-div-icon',
        html: "<div style='background-color:#E31837;color:white;padding:5px 10px;border-radius:5px;font-weight:bold;font-size:12px;'>Derco</div>",
        iconSize: [55, 20],
        iconAnchor: [27, 10]
    });
    
    var toyotaIcon = L.divIcon({
        className: 'custom-div-icon',
        html: "<div style='background-color:#EB0A1E;color:white;padding:5px 10px;border-radius:5px;font-weight:bold;font-size:12px;'>Toyota</div>",
        iconSize: [55, 20],
        iconAnchor: [27, 10]
    });
    
    // Chile - Both brands
    L.marker([-33.45, -71.5], {icon: bmwIcon}).addTo(map).bindPopup('<b>ğŸ‡¨ğŸ‡± Chile</b><br>BMW: 4 websites');
    L.marker([-35.5, -71.5], {icon: subaruIcon}).addTo(map).bindPopup('<b>ğŸ‡¨ğŸ‡± Chile</b><br>Subaru: 1 website');
    L.marker([-32.0, -71.5], {icon: dercoIcon}).addTo(map).bindPopup('<b>ğŸ‡¨ğŸ‡± Chile</b><br>Derco: 1 mystery shopper (Suzuki and Renault)');
    
    // Peru - Both brands  
    L.marker([-12.05, -78.0], {icon: bmwIcon}).addTo(map).bindPopup('<b>ğŸ‡µğŸ‡ª Peru</b><br>4 websites');
    L.marker([-14.0, -76.0], {icon: subaruIcon}).addTo(map).bindPopup('<b>ğŸ‡µğŸ‡ª Peru</b><br>1 website');
    
    // Colombia - Subaru only
    L.marker([4.7, -74.07], {icon: subaruIcon}).addTo(map).bindPopup('<b>ğŸ‡¨ğŸ‡´ Colombia</b><br>1 website');
    
    // Bulgaria - Toyota
    L.marker([42.7, 25.5], {icon: toyotaIcon}).addTo(map).bindPopup('<b>ğŸ‡§ğŸ‡¬ Bulgaria</b><br>Toyota: 3 websites (third party)');
});
</script>
```

::: {layout-ncol=4}
::: {}
![](https://img.shields.io/badge/BMW-Chile_&_Peru-0066b1?style=flat-square&logo=bmw&logoColor=white)
:::
::: {}
![](https://img.shields.io/badge/Subaru-Chile,_Peru_&_Colombia-013C74?style=flat-square&logo=subaru&logoColor=white)
:::
::: {}
![](https://img.shields.io/badge/Derco-Chile-E31837?style=flat-square&logoColor=white)
:::
::: {}
![](https://img.shields.io/badge/Toyota-Bulgaria-EB0A1E?style=flat-square&logo=toyota&logoColor=white)
:::
:::



## 6. Databricks Job

::: {.callout-success appearance="simple" icon=false}
### âœ… It's Live and Running!
The scraper runs **automatically every month** via a scheduled Databricks job â€” no manual intervention needed (except for a few tweaks).
:::

### Latest Run Stats

::: {layout-ncol=3}
::: {.card style="text-align: center;"}
### â±ï¸ Runtime
**~4 days**
106 hours (parallel)
:::
::: {.card style="text-align: center;"}
### ğŸ”¢ Parts Scraped
**~50,000**
Across all websites
:::
::: {.card style="text-align: center;"}
### ğŸŒ Websites
**5 active**
(11 planned)
:::
:::

::: {layout-ncol=2}
::: {}
**Websites being scraped sample:**

<img src="img/job_webpages.png" width="100%"/>

![BMW](https://img.shields.io/badge/BMW-4_sites-0066b1?style=flat-square&logo=bmw&logoColor=white) ![Subaru](https://img.shields.io/badge/Subaru-1_site-013C74?style=flat-square&logo=subaru&logoColor=white)

::: {.callout-note collapse="true" appearance="minimal"}
## View website list
**BMW:** bmwdirectparts.com, bmwpartshub.com, getbmwparts.com, parts.bramanmotorsbmw.com

**Subaru:** subaruparts.com
:::
:::
::: {}
**Execution time:**

<img src="img/job_time.png" width="100%"/>
:::
:::

### Built-in Reliability

| Feature | What it does |
|---------|--------------|
| ğŸ”„ **Parallel Execution** | All scrapers run simultaneously â€” adding more sites doesn't increase runtime |
| ğŸ¯ **Smart Priority** | Scrapes high-value parts first (AA â†’ AB â†’ BA...) |
| ğŸ’¾ **Checkpoint System** | Saves progress â€” resumes from last part if interrupted |
| ğŸ¢ **Ethical Pacing** | Requests at respectful intervals (no proxies needed) |

### ğŸ­ Next Optimization: Proxies

::: {.callout-tip appearance="simple" icon=false}
ğŸš€ **What if?** Proxy rotation could reduce execution time by 5-10x and significantly lower cluster costs.
:::

<details>
<summary>ğŸ“– **Learn more about proxies and cost savings**</summary>

::: {.callout-tip appearance="simple" icon=false}
## What is a proxy?
Think of it like wearing **different disguises** ğŸ¥¸ â€” instead of always showing your real identity (IP address), you rotate through many identities so websites don't recognize you're the same visitor.
:::

**Why this matters for us:**

```{mermaid}
%%| fig-width: 7
flowchart LR
    subgraph NOW ["â±ï¸ Current Setup"]
        A[1 request] --> B[Wait 2-5 sec]
        B --> C[Next request]
    end
    
    subgraph PROXY ["ğŸš€ With Proxies"]
        D[Request via IP #1]
        E[Request via IP #2]
        F[Request via IP #3]
        D --> G[No waiting needed!]
        E --> G
        F --> G
    end
    
    style NOW fill:#fff3e0,stroke:#ff9800,color:#333
    style PROXY fill:#e8f5e9,stroke:#388e3c,color:#333
    style G fill:#27ae60,color:#fff
```

::: {layout-ncol=2}
::: {.card}
#### âš¡ Faster Execution
- No need to wait between requests
- Multiple requests simultaneously
- **Potential 5-10x speed improvement**
:::
::: {.card}
#### ğŸ’° Lower Costs
- Less cluster runtime = lower compute costs
- Job finishes in hours instead of days
- **Significant Databricks Unit (DBU) savings**
:::
:::

::: {.callout-note appearance="minimal"}
ğŸ“‹ **Status:** Planned optimization â€” evaluating proxy providers for enterprise compliance.
:::

</details>

## 7. Our Own Scraping AI Assistant

::: {.callout-tip appearance="simple" icon=false}
## ğŸ¤– The Problem
As the scraper grows in complexity (more websites, more features), the codebase becomes overwhelming for new team members. Generic AI assistants built with ChatGPT struggle with this specialized code.
:::

**Our Solution:** A custom AI assistant trained on:

::: {layout-ncol=3}
::: {.card style="text-align: center;"}
![](https://img.shields.io/badge/ğŸ“‚-Our_Databricks_Code-blue?style=for-the-badge)
:::
::: {.card style="text-align: center;"}
![](https://img.shields.io/badge/ğŸ“š-Crawl4AI_Docs-green?style=for-the-badge)
:::
::: {.card style="text-align: center;"}
![](https://img.shields.io/badge/ğŸ§ -Google_Gemini-orange?style=for-the-badge)
:::
:::

::: {.callout-note appearance="minimal"}
ğŸ¯ **Result:** Fast iterations, minimal hallucinations, and code that actually works with our setup.
:::

### ğŸ’¬ Try it yourself!

Ask the assistant anything about Crawl4AI â€” how to set up crawlers, extract data, handle errors, and more. The first question takes about 5 seconds, after that responses are quicker.

```{=html}
<iframe src="https://camilo112263.shinyapps.io/crawl4ai-assistant/" 
        width="100%" height="750px" frameborder="0" 
        style="border: 1px solid #dee2e6; border-radius: 8px;">
</iframe>
```

::: {.callout-note collapse="true"}
## ğŸ”§ How the Assistant Works (click to expand)

The assistant uses **Retrieval-Augmented Generation (RAG)** to answer questions about Crawl4AI with high accuracy.

**Tech Stack:**

| Component | Tool | Purpose |
|-----------|------|---------|
| ğŸ§  **LLM** | Google Gemini 2.5 Flash | Fast, accurate responses |
| ğŸ“Š **Embeddings** | HuggingFace BGE | Convert text to vectors |
| ğŸ” **Vector Store** | LlamaIndex | Semantic search over code |
| ğŸ“¦ **Data Ingestion** | GitIngest | Fetch entire GitHub repos |

**Pipeline:** GitHub Repo â†’ GitIngest â†’ Vector Embeddings â†’ LlamaIndex Store â†’ Query Engine â†’ AI Response

**Key steps:**

1. **Ingest** â€” Fetch the entire Crawl4AI repository using GitIngest
2. **Embed** â€” Convert code and docs into vector embeddings (BGE model)
3. **Index** â€” Store vectors in a persistent LlamaIndex for fast retrieval
4. **Query** â€” User questions retrieve relevant context, then Gemini generates answers

**Custom prompt template** ensures step-by-step answers with code snippets when relevant.
:::

::: {.callout-tip appearance="minimal" icon=false}
ğŸ’¡ **What's next?** This assistant was built *before* we had access to **GitHub Copilot**. Now with Copilot in the IDE, we can feed it any context directly â€” making development even faster and more seamless.
:::

## 8. Next Steps

::: {.callout-tip appearance="simple" icon=false}
## ğŸš€ Where do we go from here?
This scraper is just the beginning â€” here's our roadmap for scaling impact.
:::

::: {layout-ncol=2}
::: {.card}
### ğŸ”¬ Phase 1: Deeper Insights
- **Competitor Pattern Analysis** â€” Use AI models to study pricing strategies, and market positioning.
- **Trend Detection** â€” Identify seasonal patterns and emerging behaviors across brands.
- **Automated Alerts** â€” Get notified when competitors make significant changes.
:::

::: {.card}
### ğŸŒ Phase 2: Scale Across Inchcape
- **Other Markets** â€” Expand to additional countries and regions.
- **Other Use Cases** â€” Adapt the tool for parts pricing, vehicles, reviews, etc.
- **Scrapers Pricing** â€” Enable business users to request new scrapers at a price.
:::
:::

```{mermaid}
%%| fig-width: 6
flowchart LR
    A[ğŸ“Š Raw Data] --> B[ğŸ¤– AI Analysis]
    B --> C[ğŸ’¡ Actionable Insights]
    C --> D[ğŸ“ˆ Business Decisions]
    
    style A fill:#e3f2fd,stroke:#1976d2,color:#333
    style B fill:#f3e5f5,stroke:#7b1fa2,color:#333
    style C fill:#fff3e0,stroke:#ff9800,color:#333
    style D fill:#e8f5e9,stroke:#388e3c,color:#333
```

::: {.callout-note appearance="minimal"}
ğŸ’¬ **Interested in using this for your team?** Reach out to the Data Science Team to discuss how we can adapt this solution for your needs.
:::

---

<center style="color: #888; font-size: 0.9em;">
*This document was created with the help of GitHub Copilot.*
</center>